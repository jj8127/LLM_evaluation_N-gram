{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating with RAG:   0%|          | 0/421 [00:00<?, ?it/s]/tmp/ipykernel_3902077/763786023.py:65: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/du3/Desktop/duchatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_3902077/763786023.py:86: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context_docs = retriever.get_relevant_documents(question)\n",
      "/tmp/ipykernel_3902077/763786023.py:108: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llama([message])\n",
      "Evaluating with RAG: 100%|██████████| 421/421 [2:57:05<00:00, 25.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 완료! 결과는 'llama_NonePEFT_RAG_new.csv'에 저장되었습니다.\n",
      "                             question  \\\n",
      "0                  산업디자인학과는 어떤 학과인가요?   \n",
      "1           산업디자인학과의 주요 학과 연혁은 무엇인가요?   \n",
      "2  산업디자인학과의 교수님들은 누구이며, 연락처는 어떻게 되나요?   \n",
      "3           산업디자인학과의 교육목표와 방침은 무엇인가요?   \n",
      "4           산업디자인학과 졸업 후 진로는 어떻게 되나요?   \n",
      "\n",
      "                                           reference  \\\n",
      "0  산업디자인학과는 창의적이고 능동적인 산업디자이너를 양성하며, 제품디자인, UI/UX...   \n",
      "1  1981년에 학과가 신설되었으며, 2021년 산업디자인학과로 명칭이 변경되었습니다....   \n",
      "2  1. 유승용 교수: 본관 5층 M503호, 031-720-2116, acapsule...   \n",
      "3  융복합 기술 시대에 대응한 현장 중심의 직무능력과 창의적 사고력을 길러내어, 국내외...   \n",
      "4  제품디자인, 환경디자인, 3D프린팅디자인, 멀티미디어디자인, UI/UX디자인 등 다...   \n",
      "\n",
      "                                           generated      bleu    meteor  \\\n",
      "0                                    صن業디자인학과가 있습니다.  0.000000  0.000000   \n",
      "1  산업디자인학과는 교육부 선정 사회맞춤형 사업의 일환으로 5년간 운영하여 현장중심의 ...  0.000000  0.000000   \n",
      "2  산업디자인학과의 교수님들은 유승용교수님, 이경아교수님, 다까스요꼬교수님, 김두한교수...  0.003051  0.055556   \n",
      "3  산업디자인학과의 교육목표는 4차 산업 혁신과 융합지식을 발전시키기 위해 항공및설계분...  0.000000  0.000000   \n",
      "4  산업디자인학과 졸업 후 진로는 다음과 같습니다.\\n\\n* 제품디자인, 환경디자인이 ...  0.025281  0.189300   \n",
      "\n",
      "    rouge_1  rouge_2   rouge_L  \n",
      "0  0.000000      0.0  0.000000  \n",
      "1  0.000000      0.0  0.000000  \n",
      "2  0.153846      0.0  0.153846  \n",
      "3  0.000000      0.0  0.000000  \n",
      "4  0.666667      0.5  0.666667  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain 관련\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.schema import Document\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# EnsembleRetriever, KiwiBM25Retriever 추가\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "\n",
    "# ollama_model_load.py에서 정의한 Ollama 모델 llama 불러오기\n",
    "from ollama_model_load import llama\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# 1) 평가 지표 계산 함수\n",
    "def calculate_bleu(reference: str, generated: str):\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "def calculate_meteor(reference: str, generated: str):\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "    return meteor_score([reference_tokens], generated_tokens)\n",
    "\n",
    "def calculate_rouge(reference: str, generated: str):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return {\n",
    "        \"rouge_1\": scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge_2\": scores[\"rouge2\"].fmeasure,\n",
    "        \"rouge_L\": scores[\"rougeL\"].fmeasure\n",
    "    }\n",
    "\n",
    "# 2) RAG 검색 수행 함수 (EnsembleRetriever 적용)\n",
    "def perform_rag(\n",
    "    question: str,\n",
    "    pdf_file: str = \"QADataset_new.pdf\",\n",
    "    chunk_size: int = 100,\n",
    "    chunk_overlap: int = 50,\n",
    "    device: str = \"cuda\"\n",
    ") -> str:\n",
    "    doc = fitz.open(pdf_file)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = [Document(page_content=t) for t in splitter.split_text(full_text)]\n",
    "\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "\n",
    "    # FAISS Retriever 생성\n",
    "    db_faiss = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    faiss_retriever = db_faiss.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "    # KiwiBM25Retriever 생성\n",
    "    kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
    "\n",
    "    # EnsembleRetriever 생성\n",
    "    retriever = EnsembleRetriever(\n",
    "        retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
    "        weights=[1, 0],\n",
    "        search_type=\"mmr\",\n",
    "    )\n",
    "\n",
    "    # 검색 결과 가져오기\n",
    "    context_docs = retriever.get_relevant_documents(question)\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in context_docs)\n",
    "    return context_text\n",
    "\n",
    "# 3) LLM 질의 함수 (RAG Prompt 포함)\n",
    "def query_llm(context: str, question: str) -> str:\n",
    "    RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    아래 정보(context)를 참고하여 사용자 질문에 답해주세요:\n",
    "    {context}\n",
    "\n",
    "    질문:\n",
    "    {question}\n",
    "\n",
    "    답변 시, 질문의 핵심만 파악하여 간결하게 1~2문장으로 답변하고, \n",
    "    불필요한 설명은 피합니다. (동서울대학교 관련 정보라면 그 내용만 요약)\n",
    "\n",
    "    답변:\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    message = HumanMessage(content=formatted_prompt)\n",
    "\n",
    "    response = llama([message])  \n",
    "    return response.content.strip()\n",
    "\n",
    "# 4) 전체 평가 함수\n",
    "def evaluate_model_responses(\n",
    "    csv_file: str = \"QADataset_old.csv\",\n",
    "    pdf_file: str = \"QADataset_new.pdf\",\n",
    "    output_file: str = \"Evaluation_RAG_results.csv\",\n",
    "    batch_size: int = 5,\n",
    "    chunk_size: int = 100,\n",
    "    chunk_overlap: int = 50,\n",
    "    device: str = \"cuda\"\n",
    "):\n",
    "    processed_count = 0\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file, encoding='utf-8-sig')\n",
    "        processed_count = len(existing_df)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file, encoding='euc-kr')\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 때 오류 발생: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(df)\n",
    "    if processed_count >= total_rows:\n",
    "        print(\"이미 모든 행이 처리되었습니다.\")\n",
    "        return pd.read_csv(output_file, encoding='utf-8-sig')\n",
    "\n",
    "    evaluation_results = []\n",
    "    for idx in tqdm(range(processed_count, total_rows), desc=\"Evaluating with RAG\"):\n",
    "        question = df.iloc[idx, 0]\n",
    "        reference = df.iloc[idx, 1]\n",
    "\n",
    "        context = perform_rag(\n",
    "            question=question,\n",
    "            pdf_file=pdf_file,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        generated_response = query_llm(context, question)\n",
    "\n",
    "        bleu_score = calculate_bleu(reference, generated_response)\n",
    "        meteor_score_value = calculate_meteor(reference, generated_response)\n",
    "        rouge_scores = calculate_rouge(reference, generated_response)\n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"reference\": reference,\n",
    "            \"generated\": generated_response,\n",
    "            \"bleu\": bleu_score,\n",
    "            \"meteor\": meteor_score_value,\n",
    "            \"rouge_1\": rouge_scores[\"rouge_1\"],\n",
    "            \"rouge_2\": rouge_scores[\"rouge_2\"],\n",
    "            \"rouge_L\": rouge_scores[\"rouge_L\"]\n",
    "        })\n",
    "\n",
    "        if (len(evaluation_results) % batch_size == 0) or (idx == total_rows - 1):\n",
    "            partial_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "            if os.path.exists(output_file) and processed_count > 0:\n",
    "                partial_df.to_csv(\n",
    "                    output_file,\n",
    "                    mode='a',\n",
    "                    index=False,\n",
    "                    header=False,\n",
    "                    encoding='utf-8-sig'\n",
    "                )\n",
    "            else:\n",
    "                partial_df.to_csv(\n",
    "                    output_file,\n",
    "                    mode='w',\n",
    "                    index=False,\n",
    "                    header=True,\n",
    "                    encoding='utf-8-sig'\n",
    "                )\n",
    "\n",
    "            evaluation_results = []\n",
    "            processed_count = idx + 1\n",
    "\n",
    "    final_df = pd.read_csv(output_file, encoding='utf-8-sig')\n",
    "    print(f\"평가 완료! 결과는 '{output_file}'에 저장되었습니다.\")\n",
    "    return final_df\n",
    "\n",
    "# 5) 메인 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # RAG 평가 실행\n",
    "    final_df = evaluate_model_responses(\n",
    "        csv_file=\"QADataset_new.csv\",     \n",
    "        pdf_file=\"QADataset_new.pdf\",     \n",
    "        output_file=\"llama_NonePEFT_RAG_new.csv\", \n",
    "        batch_size=5,\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=50,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    print(final_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

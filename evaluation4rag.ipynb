{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import RedirectResponse\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "app = FastAPI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 계산 함수\n",
    "def calculate_bleu(reference: str, generated: str):\n",
    "    reference_tokens = reference.split()  # 참고 문장 토큰화\n",
    "    generated_tokens = generated.split()  # 생성된 문장 토큰화\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "def calculate_meteor(reference: str, generated: str):\n",
    "    return meteor_score([reference], generated)\n",
    "\n",
    "def calculate_rouge(reference: str, generated: str):\n",
    "    scorer = rouge_scorer.RougeScorer(metrics=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return {\n",
    "        \"rouge_1\": scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge_2\": scores[\"rouge2\"].fmeasure,\n",
    "        \"rouge_L\": scores[\"rougeL\"].fmeasure\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 함수\n",
    "def evaluate_model_responses(csv_file: str):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        question = row[0]  # 첫 번째 열: 질문\n",
    "        reference = row[1]  # 두 번째 열: 정답(label)\n",
    "\n",
    "        # 모델의 응답을 FastAPI로 가져오기\n",
    "        response = rag_query(QueryRequest(question=question))  # FastAPI 엔드포인트 호출\n",
    "\n",
    "        generated_response = response['answer']  # FastAPI에서 반환된 응답\n",
    "\n",
    "        # BLEU, METEOR, ROUGE 계산\n",
    "        bleu_score = calculate_bleu(reference, generated_response)\n",
    "        meteor_score_value = calculate_meteor(reference, generated_response)\n",
    "        rouge_scores = calculate_rouge(reference, generated_response)\n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"reference\": reference,\n",
    "            \"generated\": generated_response,\n",
    "            \"bleu\": bleu_score,\n",
    "            \"meteor\": meteor_score_value,\n",
    "            \"rouge_1\": rouge_scores[\"rouge_1\"],\n",
    "            \"rouge_2\": rouge_scores[\"rouge_2\"],\n",
    "            \"rouge_L\": rouge_scores[\"rouge_L\"]\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/evaluate-model\")\n",
    "async def evaluate_model():\n",
    "    try:\n",
    "        # 평가 실행\n",
    "        results = evaluate_model_responses(\"evaluation.csv\")\n",
    "        return {\"evaluation_results\": results.to_dict(orient=\"records\")}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

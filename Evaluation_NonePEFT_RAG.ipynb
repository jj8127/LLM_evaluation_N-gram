{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du3/Desktop/LLM_evaluation_N-gram-main/ollama_model_load.py:6: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llama = ChatOllama(model=\"llama3.2:latest\", stop=[\"</s>\"])\n",
      "Evaluating with RAG:   0%|          | 0/770 [00:00<?, ?it/s]/tmp/ipykernel_1397494/859700076.py:63: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/du3/Desktop/duchatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1397494/859700076.py:72: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context_docs = retriever.get_relevant_documents(question)\n",
      "/tmp/ipykernel_1397494/859700076.py:95: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llama([message])\n",
      "Evaluating with RAG: 100%|██████████| 770/770 [3:49:32<00:00, 17.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 완료! 결과는 'llama_NonePEFT_RAG_old2.csv'에 저장되었습니다.\n",
      "                question                          reference  \\\n",
      "0   동서울대 컴퓨터소프트웨어과 전화번호?  컴퓨터소프트웨어과 전화번호는 031-720-2090 입니다.   \n",
      "1      동서울대 컴퓨터전자과 전화번호?     컴퓨터전자과 전화번호는 031-720-2070 입니다.   \n",
      "2       동서울대 항공기계과 전화번호?      항공기계과 전화번호는 031-720-2055 입니다.   \n",
      "3      동서울대 미래자동차과 전화번호?     미래자동차과 전화번호는 031-720-2040 입니다.   \n",
      "4       동서울대 전기공학과 전화번호?      전기공학과 전화번호는 031-720-2060 입니다.   \n",
      "\n",
      "                            generated      bleu    meteor  rouge_1  rouge_2  \\\n",
      "0                        031-720-2090  0.008854  0.135135      1.0      1.0   \n",
      "1                    031-720-2070입니다.  0.000000  0.000000      1.0      1.0   \n",
      "2  동서울대 항공기계과 전화번호는 031-720-2055 입니다.  0.668740  0.967988      1.0      1.0   \n",
      "3                    031-720-2040입니다.  0.000000  0.000000      1.0      1.0   \n",
      "4   동서울대 전기공학과 전화번호는 031-720-2060입니다.  0.169904  0.468750      1.0      1.0   \n",
      "\n",
      "   rouge_L  \n",
      "0      1.0  \n",
      "1      1.0  \n",
      "2      1.0  \n",
      "3      1.0  \n",
      "4      1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# rag_evaluation.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain 관련\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.schema import Document\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ollama_model_load.py에서 정의한 Ollama 모델 llama 불러오기\n",
    "from ollama_model_load import llama\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 1) 평가 지표 계산 함수\n",
    "def calculate_bleu(reference: str, generated: str):\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "def calculate_meteor(reference: str, generated: str):\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "    return meteor_score([reference_tokens], generated_tokens)\n",
    "\n",
    "def calculate_rouge(reference: str, generated: str):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return {\n",
    "        \"rouge_1\": scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge_2\": scores[\"rouge2\"].fmeasure,\n",
    "        \"rouge_L\": scores[\"rougeL\"].fmeasure\n",
    "    }\n",
    "\n",
    "# 2) RAG 검색 수행 함수\n",
    "def perform_rag(\n",
    "    question: str,\n",
    "    pdf_file: str = \"QADataset_new.pdf\",\n",
    "    chunk_size: int = 100,\n",
    "    chunk_overlap: int = 50,\n",
    "    device: str = \"cuda\"\n",
    ") -> str:\n",
    "    doc = fitz.open(pdf_file)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = [Document(page_content=t) for t in splitter.split_text(full_text)]\n",
    "\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "    context_docs = retriever.get_relevant_documents(question)\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in context_docs)\n",
    "    return context_text\n",
    "\n",
    "# 3) LLM 질의 함수 (RAG Prompt 포함)\n",
    "def query_llm(context: str, question: str) -> str:\n",
    "    RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    아래 정보(context)를 참고하여 사용자 질문에 답해주세요:\n",
    "    {context}\n",
    "\n",
    "    질문:\n",
    "    {question}\n",
    "\n",
    "    답변 시, 질문의 핵심만 파악하여 간결하게 1~2문장으로 답변하고, \n",
    "    불필요한 설명은 피합니다. (동서울대학교 관련 정보라면 그 내용만 요약)\n",
    "\n",
    "    답변:\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    message = HumanMessage(content=formatted_prompt)\n",
    "\n",
    "    # llama(ollama_model_load에서 import)를 사용\n",
    "    response = llama([message])  \n",
    "    return response.content.strip()\n",
    "\n",
    "# 4) 전체 평가 함수\n",
    "def evaluate_model_responses(\n",
    "    csv_file: str = \"QADataset_old.csv\",\n",
    "    pdf_file: str = \"QADataset_new.pdf\",\n",
    "    output_file: str = \"Evaluation_RAG_results.csv\",\n",
    "    batch_size: int = 5,\n",
    "    chunk_size: int = 100,\n",
    "    chunk_overlap: int = 50,\n",
    "    device: str = \"cuda\"\n",
    "):\n",
    "    processed_count = 0\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file, encoding='utf-8-sig')\n",
    "        processed_count = len(existing_df)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file, encoding='euc-kr')\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 때 오류 발생: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(df)\n",
    "    if processed_count >= total_rows:\n",
    "        print(\"이미 모든 행이 처리되었습니다.\")\n",
    "        return pd.read_csv(output_file, encoding='utf-8-sig')\n",
    "\n",
    "    evaluation_results = []\n",
    "    for idx in tqdm(range(processed_count, total_rows), desc=\"Evaluating with RAG\"):\n",
    "        question = df.iloc[idx, 0]\n",
    "        reference = df.iloc[idx, 1]\n",
    "\n",
    "        context = perform_rag(\n",
    "            question=question,\n",
    "            pdf_file=pdf_file,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        generated_response = query_llm(context, question)\n",
    "\n",
    "        bleu_score = calculate_bleu(reference, generated_response)\n",
    "        meteor_score_value = calculate_meteor(reference, generated_response)\n",
    "        rouge_scores = calculate_rouge(reference, generated_response)\n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"reference\": reference,\n",
    "            \"generated\": generated_response,\n",
    "            \"bleu\": bleu_score,\n",
    "            \"meteor\": meteor_score_value,\n",
    "            \"rouge_1\": rouge_scores[\"rouge_1\"],\n",
    "            \"rouge_2\": rouge_scores[\"rouge_2\"],\n",
    "            \"rouge_L\": rouge_scores[\"rouge_L\"]\n",
    "        })\n",
    "\n",
    "        if (len(evaluation_results) % batch_size == 0) or (idx == total_rows - 1):\n",
    "            partial_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "            if os.path.exists(output_file) and processed_count > 0:\n",
    "                partial_df.to_csv(\n",
    "                    output_file,\n",
    "                    mode='a',\n",
    "                    index=False,\n",
    "                    header=False,\n",
    "                    encoding='utf-8-sig'\n",
    "                )\n",
    "            else:\n",
    "                partial_df.to_csv(\n",
    "                    output_file,\n",
    "                    mode='w',\n",
    "                    index=False,\n",
    "                    header=True,\n",
    "                    encoding='utf-8-sig'\n",
    "                )\n",
    "\n",
    "            evaluation_results = []\n",
    "            processed_count = idx + 1\n",
    "\n",
    "    final_df = pd.read_csv(output_file, encoding='utf-8-sig')\n",
    "    print(f\"평가 완료! 결과는 '{output_file}'에 저장되었습니다.\")\n",
    "    return final_df\n",
    "\n",
    "# 5) 메인 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # RAG 평가 실행\n",
    "    final_df = evaluate_model_responses(\n",
    "        csv_file=\"QADataset_old.csv\",     \n",
    "        pdf_file=\"QADataset_new.pdf\",     \n",
    "        output_file=\"llama_NonePEFT_RAG_old2.csv\", \n",
    "        batch_size=5,\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=50,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    print(final_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_parquet('train-00000-of-00001-1ae224438dce829b.parquet')\n",
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=[\"instruction\", \"reference\", \"generated\", \"bleu_score\", \"meteor_score\", \"rouge_1\", \"rouge_2\", \"rouge_L\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load Ollama model\n",
    "def load_ollama_model(model_name):\n",
    "    # Load the Ollama model using its API\n",
    "    return model_name  # The model itself is accessed through Ollama's API\n",
    "\n",
    "def generate_response_ollama(model_name, prompt, max_length=256):\n",
    "    # Use Ollama's API to generate the response\n",
    "    response = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference: str, generated: str):\n",
    "    reference_tokens = reference.split()  # Tokenize the reference text\n",
    "    generated_tokens = generated.split()  # Tokenize the generated text\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "def calculate_meteor(reference: str, generated: str):\n",
    "    return meteor_score([reference], generated)\n",
    "\n",
    "def calculate_rouge(reference: str, generated: str):\n",
    "    scorer = rouge_scorer.RougeScorer(metrics=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return {\n",
    "        \"rouge_1\": scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge_2\": scores[\"rouge2\"].fmeasure,\n",
    "        \"rouge_L\": scores[\"rougeL\"].fmeasure\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, model_name):\n",
    "    global results_df\n",
    "    for idx in range(len(dataset)):\n",
    "        instruction = dataset.at[idx, \"instruction\"]\n",
    "        reference = dataset.at[idx, \"output\"] \n",
    "        generated_text = generate_response_ollama(model_name, instruction)\n",
    "\n",
    "        # Calculate BLEU, METEOR, and ROUGE scores\n",
    "        bleu_score = calculate_bleu(reference, generated_text)\n",
    "        meteor_score_value = calculate_meteor(reference, generated_text)\n",
    "        rouge_scores = calculate_rouge(reference, generated_text)\n",
    "\n",
    "        # Print the evaluation details\n",
    "        print(f\"Input: {instruction}\")\n",
    "        print(f\"Reference: {reference}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(f\"BLEU Score: {bleu_score}\")\n",
    "        print(f\"METEOR Score: {meteor_score_value}\")\n",
    "        print(f\"ROUGE Scores: {rouge_scores}\\n\")\n",
    "        \n",
    "        # Add results to DataFrame\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            \"instruction\": [instruction],\n",
    "            \"reference\": [reference],\n",
    "            \"generated\": [generated_text],\n",
    "            \"bleu_score\": [bleu_score],\n",
    "            \"meteor_score\": [meteor_score_value],\n",
    "            \"rouge_1\": [rouge_scores[\"rouge_1\"]],\n",
    "            \"rouge_2\": [rouge_scores[\"rouge_2\"]],\n",
    "            \"rouge_L\": [rouge_scores[\"rouge_L\"]]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_name = \"DUCChatbot5ep:latest\"  # Ollama model name\n",
    "\n",
    "    # Load Ollama model\n",
    "    model = load_ollama_model(model_name)\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluate_model(dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
